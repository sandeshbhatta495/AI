{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16481e9-837b-4be9-a3e6-592974b06a5b",
   "metadata": {},
   "source": [
    "# Why Was the Kernel Term Introduced in SVM?\n",
    "# Simple Explanation\n",
    "# Support Vector Machines (SVMs) were first designed to separate data with a straight line (linear separation). But real-world data is often messy and can't be split by a simple lineâ€”think of data points forming circles or curves. The kernel was introduced to \"trick\" SVM into handling these non-linear patterns without making the math too complicated or slow.\n",
    "# Detailed Theory\n",
    "# SVMs aim to find the best hyperplane (a boundary) that separates classes of data while maximizing the margin (distance) between the boundary and the closest points (support vectors). In the original \"hard-margin\" SVM by Vladimir Vapnik and Alexey Chervonenkis in the 1960s, this worked only for linearly separable data. By the 1990s, with contributions from Vapnik and others, the \"kernel trick\" was added to extend SVM to non-linear cases. The idea is to implicitly map data into a higher-dimensional space where it becomes linearly separable, without actually computing the high-dimensional coordinates (which could be infinite or computationally expensive).\n",
    "# Formulas\n",
    "# The basic linear SVM decision function is:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d23900e-654f-4816-871f-49dbffc0b95c",
   "metadata": {},
   "source": [
    " f({x}) = {w}^T {x} + b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae547341-8824-42a0-9ecf-15f7fbd2698b",
   "metadata": {},
   "source": [
    "# Where $\\mathbf{w}$ the weight vector, $\\mathbf{x}$ is input, and $b$ is bias.\n",
    "# With kernels, we map $\\mathbf{x}$ to a feature space via $\\phi(\\mathbf{x})$, so:\n",
    "# $ f(\\mathbf{x}) = \\mathbf{w}^T \\phi(\\mathbf{x}) + b $\n",
    "# But instead of computing $\\phi$, we use a kernel $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j) \\rangle$.\n",
    "# Real Examples\n",
    "\n",
    "# Linear data: Points of two classes separated by a line, like classifying emails as spam/non-spam based on word counts.\n",
    "# Non-linear: XOR problem (points where classes alternate like a checkerboard) or classifying images of cats vs. dogs where features curve in shape space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1936a-e0f5-48ca-a469-c5243de8d716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
